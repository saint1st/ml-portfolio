---
title: "Anomaly detection"
author: "Darkhan Islam"
date: "2024-10-26"
categories: [theory, code]
image: anomaly.jpeg
---

In this tutorial, we will implement the anomaly detection algorithm and apply it to detect failing servers on a network.


### Problem Statement

The dataset contains two features -
   * throughput (mb/s) and
   * latency (ms) of response of each server.

While your servers were operating, you collected $m=307$ examples of how they were behaving,
and thus have an unlabeled dataset
$$
\{x^{(1)}, \ldots, x^{(m)}\}
$$.
* You suspect that the vast majority of these examples are “normal” (non-anomalous) examples of the servers operating normally, but there might also be some examples of servers acting anomalously within this dataset.

You will use a Gaussian model to detect anomalous examples in your
dataset.
* You will first start on a 2D dataset that will allow you to visualize what the algorithm is doing.
* On that dataset you will fit a Gaussian distribution and then find values that have very low probability and hence can be considered anomalies.
* After that, you will apply the anomaly detection algorithm to a larger dataset with many dimensions.

    - You will use `X_train` to fit a Gaussian distribution
    - You will use `X_val` and `y_val` as a cross validation set to select a threshold and determine anomalous vs normal examples


### Gaussian distribution

To perform anomaly detection, you will first need to fit a model to the data’s distribution.

* Given a training set
$$
\{x^{(1)}, ..., x^{(m)}\}
$$
you want to estimate the Gaussian distribution for each
of the features $x_i$.

* Recall that the Gaussian distribution is given by

   $$
   p(x ; \mu,\sigma ^2) = \frac{1}{\sqrt{2 \pi \sigma ^2}}\exp^{ - \frac{(x - \mu)^2}{2 \sigma ^2} }
   $$

   where $\mu$ is the mean and $\sigma^2$ is the variance.

* For each feature $i = 1\ldots n$, you need to find parameters $\mu_i$ and $\sigma_i^2$ that fit the data in the $i$-th dimension $\{x_i^{(1)}, ..., x_i^{(m)}\}$ (the $i$-th dimension of each example).

#### Estimating parameters for a Gaussian distribution
You can estimate the parameters, ($\mu_i$, $\sigma_i^2$), of the $i$-th
feature by using the following equations. To estimate the mean, you will
use:

$$
\mu_i = \frac{1}{m} \sum_{j=1}^m x_i^{(j)$$

and for the variance you will use:
$$
\sigma_i^2 = \frac{1}{m} \sum_{j=1}^m (x_i^{(j)} - \mu_i)^2
$$

```python
def estimate_gaussian(X):
    m, n = X.shape

    mu = (1/m)*np.sum(X, axis=0)
    var = (1/m)*np.sum((X-mu)**2,axis=0)
    return mu, var
```

#### Selecting the threshold $\epsilon$

Now that you have estimated the Gaussian parameters, you can investigate which examples have a very high probability given this distribution and which examples have a very low probability.

* The low probability examples are more likely to be the anomalies in our dataset.
* One way to determine which examples are anomalies is to select a threshold based on a cross validation set.

In this section, you will complete the code in `select_threshold` to select the threshold $\varepsilon$ using the $F_1$ score on a cross validation set.

* For this, we will use a cross validation set
$$
\{(x_{\rm cv}^{(1)}, y_{\rm cv}^{(1)}),\ldots, (x_{\rm cv}^{(m_{\rm cv})}, y_{\rm cv}^{(m_{\rm cv})})\}
$$ ,
where the label $y=1$ corresponds to an anomalous example, and $y=0$ corresponds to a normal example.
* For each cross validation example, we will compute $p(x_{\rm cv}^{(i)})$. The vector of all of these probabilities $p(x_{\rm cv}^{(1)}), \ldots, p(x_{\rm cv}^{(m_{\rm cv})})$ is passed to `select_threshold` in the vector `p_val`.
* The corresponding labels $y_{\rm cv}^{(1)}, \ldots, y_{\rm cv}^{(m_{\rm cv})}$ are passed to the same function in the vector `y_val`.


* In the provided code `select_threshold`, there is already a loop that will try many different values of $\varepsilon$ and select the best $\varepsilon$ based on the $F_1$ score.

* You need to implement code to calculate the F1 score from choosing `epsilon` as the threshold and place the value in `F1`.

  * Recall that if an example $x$ has a low probability $p(x) < \varepsilon$, then it is classified as an anomaly.

  * Then, you can compute precision and recall by:
   $$
   \begin{aligned}
   prec&=&\frac{tp}{tp+fp}\\
   rec&=&\frac{tp}{tp+fn},
   \end{aligned}
   $$ where
    * $tp$ is the number of true positives: the ground truth label says it’s an anomaly and our algorithm correctly classified it as an anomaly.
    * $fp$ is the number of false positives: the ground truth label says it’s not an anomaly, but our algorithm incorrectly classified it as an anomaly.
    * $fn$ is the number of false negatives: the ground truth label says it’s an anomaly, but our algorithm incorrectly classified it as not being anomalous.

  * The $F_1$ score is computed using precision ($prec$) and recall ($rec$) as follows:
    $$
    F_1 = \frac{2\cdot prec \cdot rec}{prec + rec}
    $$


```python
def select_threshold(y_val, p_val):
    best_epsilon = 0
    best_F1 = 0
    F1 = 0
    step_size = (max(p_val) - min(p_val)) / 1000
    for epsilon in np.arange(min(p_val), max(p_val), step_size):

        predictions = (p_val < epsilon)

        tp = np.sum((predictions == 1) & (y_val == 1))
        fp = np.sum((predictions == 1) & (y_val == 0))
        fn = np.sum((predictions == 0) & (y_val == 1))

        prec = tp / (tp + fp)
        rec = tp / (tp + fn)
        F1 = 2 * prec * rec / (prec + rec)
        if F1 > best_F1:
            best_F1 = F1
            best_epsilon = epsilon
    return best_epsilon, best_F1
```
