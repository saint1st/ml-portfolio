[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here are selected projects. Each entry is a Markdown file in projects/.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLow-Light Pedestrian Detection\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post6/index.html",
    "href": "posts/post6/index.html",
    "title": "Image compression with K-Means",
    "section": "",
    "text": "The K-means algorithm is a method to automatically cluster similar data points together.\n\nConcretely, you are given a training set \\(\\{x^{(1)}, ..., x^{(m)}\\}\\), and you want to group the data into a few cohesive ‚Äúclusters‚Äù.\nK-means is an iterative procedure that\n\nStarts by guessing the initial centroids, and then\nRefines this guess by\n\nRepeatedly assigning examples to their closest centroids, and then\nRecomputing the centroids based on the assignments.\n\n\nThe \\(K\\)-means algorithm will always converge to some final set of means for the centroids.\nHowever, the converged solution may not always be ideal and depends on the initial setting of the centroids.\n\nTherefore, in practice the K-means algorithm is usually run a few times with different random initializations.\nOne way to choose between these different solutions from different random initializations is to choose the one with the lowest cost function value (distortion).\n\n\nfind_closest_centroids. * This function takes the data matrix X and the locations of all centroids inside centroids * It should output a one-dimensional array idx (which has the same number of elements as X) that holds the index of the closest centroid (a value in \\(\\{0,...,K-1\\}\\), where \\(K\\) is total number of centroids) to every training example . (Note: The index range 0 to K-1 varies slightly from what is shown in the lectures (i.e.¬†1 to K) because Python list indices start at 0 instead of 1) * Specifically, for every example \\(x^{(i)}\\) we set \\[\nc^{(i)} := j \\quad \\mathrm{that \\; minimizes} \\quad ||x^{(i)} - \\mu_j||^2,\n\\] where * \\(c^{(i)}\\) is the index of the centroid that is closest to \\(x^{(i)}\\) (corresponds to idx[i] in the starter code), and * \\(\\mu_j\\) is the position (value) of the \\(j\\)‚Äôth centroid. (stored in centroids in the starter code) * \\(||x^{(i)} - \\mu_j||\\) is the L2-norm\ndef find_closest_centroids(X, centroids):\n    # Set K\n    K = centroids.shape[0]\n    # You need to return the following variables correctly\n    idx = np.zeros(X.shape[0], dtype=int)\n\n    for i in range(X.shape[0]):\n        # Array to hold distance between X[i] and each centroids[j]\n        distance = []\n        for j in range(centroids.shape[0]):\n            norm_ij = np.linalg.norm(X[i] - centroids[j])\n            distance.append(norm_ij)\n\n        idx[i] = np.argmin(distance)\n    return idx\n# Select an initial set of centroids (3 Centroids)\ninitial_centroids = np.array([[3,3], [6,2], [8,5]])\n\n# Find closest centroids using initial_centroids\nidx = find_closest_centroids(X, initial_centroids)\n\n# Print closest centroids for the first three elements\nprint(\"First three elements in idx are:\", idx[:3])\n\nSpecifically, for every centroid \\(\\mu_k\\) we set \\[\n\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}\n\\]\nwhere\n\n\\(C_k\\) is the set of examples that are assigned to centroid \\(k\\)\n\\(|C_k|\\) is the number of examples in the set \\(C_k\\)\n\nConcretely, if two examples say \\(x^{(3)}\\) and \\(x^{(5)}\\) are assigned to centroid \\(k=2\\), then you should update \\(\\mu_2 = \\frac{1}{2}(x^{(3)}+x^{(5)})\\).\n\ndef compute_centroids(X, idx, K):\n    # Useful variables\n    m, n = X.shape\n\n    # You need to return the following variables correctly\n    centroids = np.zeros((K, n))\n\n    for k in range(K):\n        points = X[idx==k]\n        centroids[k] = np.mean(points, axis=0)\n    return centroids\nK = 3\ncentroids = compute_centroids(X, idx, K)\n\nprint(\"The centroids are:\", centroids)\nTo randomly initialize centroids\ndef kMeans_init_centroids(X, K):\n    # Randomly reorder the indices of examples\n    randidx = np.random.permutation(X.shape[0])\n\n    # Take the first K examples as centroids\n    centroids = X[randidx[:K]]\n    return centroids"
  },
  {
    "objectID": "posts/post6/index.html#implementing-k-means",
    "href": "posts/post6/index.html#implementing-k-means",
    "title": "Image compression with K-Means",
    "section": "",
    "text": "The K-means algorithm is a method to automatically cluster similar data points together.\n\nConcretely, you are given a training set \\(\\{x^{(1)}, ..., x^{(m)}\\}\\), and you want to group the data into a few cohesive ‚Äúclusters‚Äù.\nK-means is an iterative procedure that\n\nStarts by guessing the initial centroids, and then\nRefines this guess by\n\nRepeatedly assigning examples to their closest centroids, and then\nRecomputing the centroids based on the assignments.\n\n\nThe \\(K\\)-means algorithm will always converge to some final set of means for the centroids.\nHowever, the converged solution may not always be ideal and depends on the initial setting of the centroids.\n\nTherefore, in practice the K-means algorithm is usually run a few times with different random initializations.\nOne way to choose between these different solutions from different random initializations is to choose the one with the lowest cost function value (distortion).\n\n\nfind_closest_centroids. * This function takes the data matrix X and the locations of all centroids inside centroids * It should output a one-dimensional array idx (which has the same number of elements as X) that holds the index of the closest centroid (a value in \\(\\{0,...,K-1\\}\\), where \\(K\\) is total number of centroids) to every training example . (Note: The index range 0 to K-1 varies slightly from what is shown in the lectures (i.e.¬†1 to K) because Python list indices start at 0 instead of 1) * Specifically, for every example \\(x^{(i)}\\) we set \\[\nc^{(i)} := j \\quad \\mathrm{that \\; minimizes} \\quad ||x^{(i)} - \\mu_j||^2,\n\\] where * \\(c^{(i)}\\) is the index of the centroid that is closest to \\(x^{(i)}\\) (corresponds to idx[i] in the starter code), and * \\(\\mu_j\\) is the position (value) of the \\(j\\)‚Äôth centroid. (stored in centroids in the starter code) * \\(||x^{(i)} - \\mu_j||\\) is the L2-norm\ndef find_closest_centroids(X, centroids):\n    # Set K\n    K = centroids.shape[0]\n    # You need to return the following variables correctly\n    idx = np.zeros(X.shape[0], dtype=int)\n\n    for i in range(X.shape[0]):\n        # Array to hold distance between X[i] and each centroids[j]\n        distance = []\n        for j in range(centroids.shape[0]):\n            norm_ij = np.linalg.norm(X[i] - centroids[j])\n            distance.append(norm_ij)\n\n        idx[i] = np.argmin(distance)\n    return idx\n# Select an initial set of centroids (3 Centroids)\ninitial_centroids = np.array([[3,3], [6,2], [8,5]])\n\n# Find closest centroids using initial_centroids\nidx = find_closest_centroids(X, initial_centroids)\n\n# Print closest centroids for the first three elements\nprint(\"First three elements in idx are:\", idx[:3])\n\nSpecifically, for every centroid \\(\\mu_k\\) we set \\[\n\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}\n\\]\nwhere\n\n\\(C_k\\) is the set of examples that are assigned to centroid \\(k\\)\n\\(|C_k|\\) is the number of examples in the set \\(C_k\\)\n\nConcretely, if two examples say \\(x^{(3)}\\) and \\(x^{(5)}\\) are assigned to centroid \\(k=2\\), then you should update \\(\\mu_2 = \\frac{1}{2}(x^{(3)}+x^{(5)})\\).\n\ndef compute_centroids(X, idx, K):\n    # Useful variables\n    m, n = X.shape\n\n    # You need to return the following variables correctly\n    centroids = np.zeros((K, n))\n\n    for k in range(K):\n        points = X[idx==k]\n        centroids[k] = np.mean(points, axis=0)\n    return centroids\nK = 3\ncentroids = compute_centroids(X, idx, K)\n\nprint(\"The centroids are:\", centroids)\nTo randomly initialize centroids\ndef kMeans_init_centroids(X, K):\n    # Randomly reorder the indices of examples\n    randidx = np.random.permutation(X.shape[0])\n\n    # Take the first K examples as centroids\n    centroids = X[randidx[:K]]\n    return centroids"
  },
  {
    "objectID": "posts/post6/index.html#image-compression-with-k-means",
    "href": "posts/post6/index.html#image-compression-with-k-means",
    "title": "Image compression with K-Means",
    "section": "Image compression with K-means",
    "text": "Image compression with K-means\n\nIn a straightforward 24-bit color representation of an image\\(^{2}\\), each pixel is represented as three 8-bit unsigned integers (ranging from 0 to 255) that specify the red, green and blue intensity values. This encoding is often refered to as the RGB encoding.\nOur image contains thousands of colors, and in this part of the exercise, you will reduce the number of colors to 16 colors.\nBy making this reduction, it is possible to represent (compress) the photo in an efficient way.\nSpecifically, you only need to store the RGB values of the 16 selected colors, and for each pixel in the image you now need to only store the index of the color at that location (where only 4 bits are necessary to represent 16 possibilities).\n\nIn this part, you will use the K-means algorithm to select the 16 colors that will be used to represent the compressed image. * Concretely, you will treat every pixel in the original image as a data example and use the K-means algorithm to find the 16 colors that best group (cluster) the pixels in the 3- dimensional RGB space. * Once you have computed the cluster centroids on the image, you will then use the 16 colors to replace the pixels in the original image.\n# Load an image\noriginal_img = plt.imread('bird.png')\nprint(\"Shape of original_img is:\", original_img.shape)\nShape of original_img is: (128, 128, 3)\nAs you can see, this creates a three-dimensional matrix original_img where * the first two indices identify a pixel position, and * the third index represents red, green, or blue.\nFor example, original_img[50, 33, 2] gives the blue intensity of the pixel at row 50 and column 33.\nTo call the run_kMeans, you need to first transform the matrix original_img into a two-dimensional matrix.\n\nThe code below reshapes the matrix original_img to create an \\(m \\times 3\\) matrix of pixel colors (where \\(m=16384 = 128\\times128\\))\n\nX_img = np.reshape(original_img, (original_img.shape[0] * original_img.shape[1], 3))\n ### K-Means on image pixels\nNow, run the cell below to run K-Means on the pre-processed image.\n# Run your K-Means algorithm on this data\n# You should try different values of K and max_iters here\nK = 16\nmax_iters = 10\n\n# Using the function you have implemented above.\ninitial_centroids = kMeans_init_centroids(X_img, K)\n\n# Run K-Means - this can take a couple of minutes depending on K and max_iters\ncentroids, idx = run_kMeans(X_img, initial_centroids, max_iters)\n\nCompress the image\nAfter finding the top \\(K=16\\) colors to represent the image, you can now assign each pixel position to its closest centroid using the find_closest_centroids function. * This allows you to represent the original image using the centroid assignments of each pixel. * Notice that you have significantly reduced the number of bits that are required to describe the image. * The original image required 24 bits (i.e.¬†8 bits x 3 channels in RGB encoding) for each one of the \\(128\\times128\\) pixel locations, resulting in total size of \\(128 \\times 128 \\times 24 = 393,216\\) bits. * The new representation requires some overhead storage in form of a dictionary of 16 colors, each of which require 24 bits, but the image itself then only requires 4 bits per pixel location. * The final number of bits used is therefore \\(16 \\times 24 + 128 \\times 128 \\times 4 = 65,920\\) bits, which corresponds to compressing the original image by about a factor of 6.\n# Find the closest centroid of each pixel\nidx = find_closest_centroids(X_img, centroids)\n\n# Replace each pixel with the color of the closest centroid\nX_recovered = centroids[idx, :]\n\n# Reshape image into proper dimensions\nX_recovered = np.reshape(X_recovered, original_img.shape)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "My name is Darkhan Islam. I am first-year master at Politecnico di Milano studying Computer Science & Engineering. I‚Äôm Machine Learning Engineer."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Gradient Descent, how does it work?",
    "section": "",
    "text": "Gradient Descent is a method for unconstrained math optimization. It is a first order iterative algorithm for minimizing differentiable multivariate function. The key idea is to take repeated steps in the opposite direction of the gradient of the function at the current point, because this is a direction of steepest descent. ‚Äì&gt; Conversely, stepping in the direction of the gradient will lead to a trajectory that maximizes that function, the procedure is then know as gradient ascent\nIn this tutorial we will automate the process of optimizing \\(w\\) and \\(b\\) usig gradient descent"
  },
  {
    "objectID": "posts/post-with-code/index.html#problem-statement",
    "href": "posts/post-with-code/index.html#problem-statement",
    "title": "Gradient Descent, how does it work?",
    "section": "Problem Statement",
    "text": "Problem Statement\nLet‚Äôs use the same two data points as before - a house with 1000 square feet sold for \\$300,000 and a house with 2000 square feet sold for \\$500,000.\n\n\n\nSize (1000 sqft)\nPrice (1000s of dollars)\n\n\n\n\n1\n300\n\n\n2\n500\n\n\n\nx_train = np.array([1.0, 2.0])\ny_train = np.array([300.0, 500.0])"
  },
  {
    "objectID": "posts/post-with-code/index.html#define-cost-function",
    "href": "posts/post-with-code/index.html#define-cost-function",
    "title": "Gradient Descent, how does it work?",
    "section": "Define cost function",
    "text": "Define cost function\nWe consider a linear model that predicts \\(f_{w,b}(x^{(i)})\\): \\[f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}\\] In linear regression, we utilize input training data to fit the parameters \\(w\\),\\(b\\) by minimizing a measure of the error between our predictions \\(f_{w,b}(x^{(i)})\\) and the actual data \\(y^{(i)}\\). The measure is called the \\(cost\\), \\(J(w,b)\\). In training we measure the cost over all of our training samples \\(x^{(i)},y^{(i)}\\) \\[J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}\\]\nLet‚Äôs define cost function described above.\ndef compute_cost(x, y, w, b):\n    m = x.shape[0]\n    cost = 0\n\n    for i in range(m):\n        f_wb = w * x[i] + b\n        cost = cost + (f_wb - y[i])**2\n    total_cost = 1 / (2 * m) * cost\n    return total_cost"
  },
  {
    "objectID": "posts/post-with-code/index.html#gradient-descent-code-implementation",
    "href": "posts/post-with-code/index.html#gradient-descent-code-implementation",
    "title": "Gradient Descent, how does it work?",
    "section": "Gradient descent code implementation",
    "text": "Gradient descent code implementation\n\\[\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline\nb &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n\\end{align*}\\] where, parameters \\(w\\), \\(b\\) are updated simultaneously. The gradient is defined as: \\[\n\\begin{align}\n\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n\\end{align}\n\\]\nHere simultaniously means that you calculate the partial derivatives for all the parameters before updating any of the parameters.\nWe will implement GD algorithm for one feature. We need three functions:\n\ncompute_gradient implementing equation (4) and (5) above\ncompute_cost implementing equation (2) above (code from previous lab)\ngradient_descent, utilizing compute_gradient and compute_cost\n\ndef compute_gradient(x, y, w, b):\n   m = x.shape[0]\n   dj_dw = 0\n   dj_db = 0\n\n   for i in range(m):\n       f_wb = w * x[i] + b\n       dw_dw_i = (f_wb - y[i]) * x[i]\n       dj_db_i = (f_wb - y[i])\n\n       dj_db += dj_db_i\n       dj_dw += dj_dw_i\n   dj_dw = dj_dw / m\n   dj_db = dj_db / m\n\n   return dj_dw, dj_db\n\nAbove, the left plot shows \\(\\frac{\\partial J(w,b)}{\\partial w}\\) or the slope of the cost curve relative to \\(w\\) at three points. On the right side of the plot, the derivative is positive, while on the left it is negative. Due to the ‚Äòbowl shape‚Äô, the derivatives will always lead gradient descent toward the bottom where the gradient is zero.\nThe left plot has fixed \\(b=100\\). Gradient descent will utilize both \\(\\frac{\\partial J(w,b)}{\\partial w}\\) and \\(\\frac{\\partial J(w,b)}{\\partial b}\\) to update parameters. The ‚Äòquiver plot‚Äô on the right provides a means of viewing the gradient of both parameters. The arrow sizes reflect the magnitude of the gradient at that point. The direction and slope of the arrow reflects the ratio of \\(\\frac{\\partial J(w,b)}{\\partial w}\\) and \\(\\frac{\\partial J(w,b)}{\\partial b}\\) at that point. Note that the gradient points away from the minimum. Review equation (3) above. The scaled gradient is subtracted from the current value of \\(w\\) or \\(b\\). This moves the parameter in a direction that will reduce cost.\nNow that gradients can be computed, gradient descent, described in equation (3) above can be implemented below in gradient_descent.\ndef gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n    J_history = []\n    p_history = []\n    b = b_in\n    w = w_in\n\n    for i in range(num_iters):\n        dj_dw, dj_db = gradient_function(x, y, w, b)\n\n        b = b - alpha * dj_db\n        w = w - alpha * dj_dw\n\n        if i&lt;100000:\n            J_history.append(cost_function(x, y, w, b))\n            p.history_append([w,b])\n\n        if i%math.ceil(num_iters/10) ==0:\n            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db:0.3e} \",\n                  f\"w: {w:0.3e}, b:{b:0.5e}\")\n\n    return w, b, J_history, p_history"
  },
  {
    "objectID": "posts/post-with-code/index.html#training-with-gd",
    "href": "posts/post-with-code/index.html#training-with-gd",
    "title": "Gradient Descent, how does it work?",
    "section": "Training with GD",
    "text": "Training with GD\n# initialize parameters\nw_init = 0\nb_init = 0\n# some gradient descent settings\niterations = 10000\ntmp_alpha = 1.0e-2\n# run gradient descent\nw_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,\n                                                    iterations, compute_cost, compute_gradient)\nprint(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")\nTake a look at the training process by running this code. You will notice that \\(dj_dw\\) and \\(dj_db\\) get smaller, rapidly at first and then more slowly. As the process nears the ‚Äòbottom of the bowl‚Äô progress is slower due to the smaller value of the derivative at that point.\nNow that you have discovered the optimal values for the parameters \\(w\\) and \\(b\\), you can now use the model to predict housing values based on our learned parameters. As expected, the predicted values are nearly the same as the training values for the same housing. Further, the value not in the prediction is in line with the expected value.\nprint(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\nprint(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\nprint(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")\nI we take a look at the progress of gradient descent during its execution by plotting the cost over iterations on a contour plot of the cost(w,b), we will see:\n\nAbove, the contour plot shows the \\(cost(w,b)\\) over a range of \\(w\\) and \\(b\\). Cost levels are represented by the rings. Overlayed, using red arrows, is the path of gradient descent. Here are some things to note: - The path makes steady (monotonic) progress toward its goal. - initial steps are much larger than the steps near the goal"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "ML Notes & Projects\n\n\nAlgorithms explained, experiments reproduced, and tiny demos.\n\n\nWelcome! I write about the math behind algorithms, minimal PyTorch/NumPy reference implementations, and reproducible experiments. Below are the latest posts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Variable Linear Regression\n\n\n\ntheory\n\ncode\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent, how does it work?\n\n\n\ntheory\n\ncode\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks for Handwritten Digit Recognition with Binary and Multiple outputs\n\n\n\ntheory\n\ncode\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nImage compression with K-Means\n\n\n\ntheory\n\ncode\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nAnomaly detection\n\n\n\ntheory\n\ncode\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Decision Tree from scratch\n\n\n\ntheory\n\ncode\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\nImage compression with K-Means\n\n\n\ntheory\n\ncode\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2024\n\n1 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/lung-cancer.html",
    "href": "projects/lung-cancer.html",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "",
    "text": "Lung cancer remains among the most lethal malignancies ‚Äî only about one in four patients survive five years. For those with locally advanced Stage III NSCLC, radiotherapy (RT) is a primary treatment, yet cardiac exposure during RT may silently shape survival outcomes.\nThis project analyzes how spatial dose distribution to the heart correlates with 2-year overall survival (OS). Using voxel-wise statistical modeling, unsupervised clustering, and advanced visualization, we identify cardiac regions whose radiation dose patterns most strongly associate with patient outcomes."
  },
  {
    "objectID": "projects/lung-cancer.html#overview",
    "href": "projects/lung-cancer.html#overview",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "",
    "text": "Lung cancer remains among the most lethal malignancies ‚Äî only about one in four patients survive five years. For those with locally advanced Stage III NSCLC, radiotherapy (RT) is a primary treatment, yet cardiac exposure during RT may silently shape survival outcomes.\nThis project analyzes how spatial dose distribution to the heart correlates with 2-year overall survival (OS). Using voxel-wise statistical modeling, unsupervised clustering, and advanced visualization, we identify cardiac regions whose radiation dose patterns most strongly associate with patient outcomes."
  },
  {
    "objectID": "projects/lung-cancer.html#research-challenge",
    "href": "projects/lung-cancer.html#research-challenge",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "‚ö†Ô∏è Research Challenge",
    "text": "‚ö†Ô∏è Research Challenge\nRadiotherapy aims to ‚Äì maximize dose to tumor tissue, ‚Äì minimize dose to healthy organs.\nHowever, even small incidental doses to the heart and major vessels can increase mortality risk. Our working hypothesis: specific cardiac subregions receiving higher dose may correlate with lower OS."
  },
  {
    "objectID": "projects/lung-cancer.html#dataset",
    "href": "projects/lung-cancer.html#dataset",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "üß¨ Dataset",
    "text": "üß¨ Dataset\n\nInstitution: IRCCS Istituto Nazionale dei Tumori di Milano\nPatients: 321 across 5 centers\nModalities: DICOM CT scans + RT dose maps\nOutcome: 2-year overall survival\nFeatures: 54 clinical and treatment variables\n\n\n\nAnatomic / Metabolic: age, BMI, hypertension, KPS, smoking\nCancer-Related: stage, tumor size, site, mutations\nTherapy: chemotherapy cycles, immunotherapy, machine type, EQD2 dose\nOutcomes: cardiac and pulmonary events, 2-year OS"
  },
  {
    "objectID": "projects/lung-cancer.html#feature-engineering-preprocessing",
    "href": "projects/lung-cancer.html#feature-engineering-preprocessing",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "‚öôÔ∏è Feature Engineering & Preprocessing",
    "text": "‚öôÔ∏è Feature Engineering & Preprocessing\n\nMedication count: grouped as 0 / 1 / 2 / &gt;3 classes\nTherapy types: combined target + immunotherapy into one variable\nSmoking: binary encoded\nPerformance status (KPS): binned (‚â§80, 90, 100)\nChemotherapy schedule: rescaled so 0 = no therapy"
  },
  {
    "objectID": "projects/lung-cancer.html#statistical-methods",
    "href": "projects/lung-cancer.html#statistical-methods",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "üß™ Statistical Methods",
    "text": "üß™ Statistical Methods\n\nWelch‚Äôs t-test (voxel-wise analysis)\nBenjamini‚ÄìHochberg FDR correction\nPCA + DBSCAN for outlier detection\nMean-dose voxel mapping"
  },
  {
    "objectID": "projects/lung-cancer.html#methodology",
    "href": "projects/lung-cancer.html#methodology",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "üß† Methodology",
    "text": "üß† Methodology\n\nPreprocess CT + dose data into voxel grids.\nSpatial registration ‚Üí map each patient onto a shared template.\nVoxel-Based Analysis (VBA) ‚Üí compare dose voxels between survivors vs non-survivors.\nUnsupervised clustering ‚Üí detect spatial patterns of significance.\nVisualization ‚Üí render p-value and dose maps for interpretation."
  },
  {
    "objectID": "projects/lung-cancer.html#analysis-workflow",
    "href": "projects/lung-cancer.html#analysis-workflow",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "üìà Analysis Workflow",
    "text": "üìà Analysis Workflow\n\n\n\n\n\nEach patient‚Äôs dose map is warped to a common template. Voxel-wise comparisons produce p-value maps that highlight regions where dose differences predict 2-year OS."
  },
  {
    "objectID": "projects/lung-cancer.html#key-visualizations",
    "href": "projects/lung-cancer.html#key-visualizations",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "üìä Key Visualizations",
    "text": "üìä Key Visualizations\n\n\n\n\n\n\n\nConcept\nVisualization\n\n\n\n\nRaw Data\n\n\n\nDose Distribution Comparison\n Shows inter-patient dose variation.\n\n\nGroup Dose Characteristics\n Survivors show more zero-dose voxels and focused dosing.\n\n\nAxial Dose Heatmap\n Spatial asymmetry visible between left and right lung regions.\n\n\nROI Cropping & EQD‚ÇÇ Normalization\n Uses Œ±/Œ≤ = 2 and 5 to adjust dose biologically.\n\n\nSpatial Clustering of Significant Voxels\n Clusters #39-47 align with heart substructures.\n\n\nDose & Outlier Summary\n Histograms compare mean dose and zero-dose fraction by outcome group."
  },
  {
    "objectID": "projects/lung-cancer.html#findings",
    "href": "projects/lung-cancer.html#findings",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "üîç Findings",
    "text": "üîç Findings\n\nSurvivors ‚Üí more focused, precise dose distributions in cardiac voxels.\nNon-survivors ‚Üí broader exposure and less localized energy deposition.\nNo single heart substructure fully explains the effect; patterns are spatially distributed.\nProper template alignment and EQD‚ÇÇ correction are critical for reliable statistics."
  },
  {
    "objectID": "projects/lung-cancer.html#goals-implications",
    "href": "projects/lung-cancer.html#goals-implications",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "üöÄ Goals & Implications",
    "text": "üöÄ Goals & Implications\n\nClarify mechanisms of radiation-induced cardiac toxicity.\nIdentify spatial biomarkers linked to survival.\nSupport more personalized radiotherapy planning by mapping risk regions within the heart."
  },
  {
    "objectID": "projects/lung-cancer.html#academic-collaboration",
    "href": "projects/lung-cancer.html#academic-collaboration",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "üéì Academic Collaboration",
    "text": "üéì Academic Collaboration\n\nDeveloped within the Applied Statistics course at Politecnico di Milano (PoliMi, Italy). Instructor: Prof.¬†Piercesare Secchi Supervisor: Ing. Guillaume Koechlin\n\nTeam\n\nStefano Beraldo\nDarkhan Islam\nAmine Ould Hocine\nSamuel Povoa\n\nResearch Partners: Fondazione IRCCS Istituto Nazionale dei Tumori (Milan)"
  },
  {
    "objectID": "projects/lung-cancer.html#references",
    "href": "projects/lung-cancer.html#references",
    "title": "Voxel-Wise Analysis of Cardiac Dose in Lung Cancer Radiotherapy",
    "section": "üìö References",
    "text": "üìö References\n\nMcWilliam et al., Novel Methodology to Investigate the Effect of Radiation Dose to Heart Substructures on Overall Survival (2020)\nRancati et al., SLiC Algorithm for Spatial Dose Analysis, Journal of the European Society for Radiotherapy and Oncology"
  },
  {
    "objectID": "projects/pedestrian-detection.html",
    "href": "projects/pedestrian-detection.html",
    "title": "Low-Light Pedestrian Detection",
    "section": "",
    "text": "Pedestrian detection in low-light conditions is notoriously difficult ‚Äî tiny, noisy, and poorly labeled targets often fool even modern detectors.\nThis project tackles that challenge using YOLOv8 models trained on the Find Person in the Dark (LLVIP subset) Kaggle dataset, with an emphasis on data hygiene and ensemble precision.\nBy designing a scene-aware, leak-free data split, optimizing inference parameters, and combining detectors via Weighted Box Fusion (WBF), I achieved consistent improvements over baseline YOLOv8 models.\n\nKey Insight: Clean validation splits and tuned inference parameters outperform brute-force augmentation and model scaling."
  },
  {
    "objectID": "projects/pedestrian-detection.html#overview",
    "href": "projects/pedestrian-detection.html#overview",
    "title": "Low-Light Pedestrian Detection",
    "section": "",
    "text": "Pedestrian detection in low-light conditions is notoriously difficult ‚Äî tiny, noisy, and poorly labeled targets often fool even modern detectors.\nThis project tackles that challenge using YOLOv8 models trained on the Find Person in the Dark (LLVIP subset) Kaggle dataset, with an emphasis on data hygiene and ensemble precision.\nBy designing a scene-aware, leak-free data split, optimizing inference parameters, and combining detectors via Weighted Box Fusion (WBF), I achieved consistent improvements over baseline YOLOv8 models.\n\nKey Insight: Clean validation splits and tuned inference parameters outperform brute-force augmentation and model scaling."
  },
  {
    "objectID": "projects/pedestrian-detection.html#dataset-split-strategy",
    "href": "projects/pedestrian-detection.html#dataset-split-strategy",
    "title": "Low-Light Pedestrian Detection",
    "section": "Dataset & Split Strategy",
    "text": "Dataset & Split Strategy\nDataset: Find Person in the Dark ‚Äì Kaggle\n- 15,030 visible-light images (11,782 training, 3,248 testing)\n- Subset of LLVIP: A Visible-Infrared Paired Dataset for Low-Light Vision\n\nLeak-Free Split\nTo prevent near-duplicate leakage: - Grouped similar frames via perceptual hash (pHash) - Split by group to ensure scene disjointness - Verified with visual similarity inspection"
  },
  {
    "objectID": "projects/pedestrian-detection.html#results-summary",
    "href": "projects/pedestrian-detection.html#results-summary",
    "title": "Low-Light Pedestrian Detection",
    "section": "Results Summary",
    "text": "Results Summary\n\n\n\nModel\nAP@0.5\nAP@0.75\nFinal Score (‚Üì better)\n\n\n\n\nWBF Ensemble (Best)\n0.8772\n0.5634\n0.2797\n\n\nYOLOv8s Fine-Tuned\n0.9025\n0.5348\n0.2813\n\n\nYOLOv8s Heavy Aug\n0.8800\n0.5071\n0.3064\n\n\nYOLOv8n Baseline\n0.8468\n0.4839\n0.3347\n\n\n\n\nThe best results came from the ensemble, which fused predictions from three cleanly trained YOLOv8 variants.\nDespite slightly lower AP@0.5, the ensemble yielded a better final score ‚Äî showing tighter localization and fewer false positives."
  },
  {
    "objectID": "projects/pedestrian-detection.html#model-evolution",
    "href": "projects/pedestrian-detection.html#model-evolution",
    "title": "Low-Light Pedestrian Detection",
    "section": "Model Evolution",
    "text": "Model Evolution\n\n\n50 epochs, imgsz=640, batch=16\nServed as control; trained on random split\nResult: AP@0.5=0.8468, Score 0.3347\n\n\n\n\nAdded CLAHE preprocessing, high hsv_v, strong mosaic\n\nImproved recall, but noisy augmentations hurt stability\nResult: AP@0.5=0.8800, Score 0.3064\n\n\n\n\nFine-tuned from Model #2 on clean data\n\nTuned conf=0.10, iou=0.55\nResult: AP@0.5=0.9025, AP@0.75=0.5348, Score 0.2813"
  },
  {
    "objectID": "projects/pedestrian-detection.html#ensemble-method-weighted-boxes-fusion",
    "href": "projects/pedestrian-detection.html#ensemble-method-weighted-boxes-fusion",
    "title": "Low-Light Pedestrian Detection",
    "section": "Ensemble Method (Weighted Boxes Fusion)",
    "text": "Ensemble Method (Weighted Boxes Fusion)\nA multi-model fusion pipeline combining predictions based on overlap consensus.\nSteps included: 1. Per-model JSON export\n2. Weighted Boxes Fusion\n3. Post-filtering & NMS\n4. Grid search over IoU / score / consensus thresholds\n5. Evaluation at IoU 0.5 and 0.75\nBest configuration: IOU_THR=0.65 SKIP_BOX_THR=0.005 FINAL_SCORE_THR=0.20 FINAL_NMS_IOU=0.55 MIN_MODELS=1 MAX_DETS=5"
  },
  {
    "objectID": "projects/pedestrian-detection.html#qualitative-results",
    "href": "projects/pedestrian-detection.html#qualitative-results",
    "title": "Low-Light Pedestrian Detection",
    "section": "Qualitative Results",
    "text": "Qualitative Results\n\n\n\n\n\nGreen = ground truth, Red = predictions.\nCommon patterns: - Strong TPs: Clear, high-confidence boxes\n- FPs: Hallucinations on bright shadows or tree trunks\n- FNs: Tiny or occluded figures in dark corners"
  },
  {
    "objectID": "projects/pedestrian-detection.html#lessons-learned",
    "href": "projects/pedestrian-detection.html#lessons-learned",
    "title": "Low-Light Pedestrian Detection",
    "section": "Lessons Learned",
    "text": "Lessons Learned\n\nData quality &gt; fancy models ‚Äî leak-free splits alone gave &gt;10% mAP boost.\n\nInference tuning pays off ‚Äî adjusting confidence/IoU thresholds improved Kaggle score by ~5%.\n\nWBF outperforms naive NMS ‚Äî reduces duplicates, stabilizes localization.\n\nAugmentation isn‚Äôt magic ‚Äî aggressive color jitter or CLAHE can make labels inconsistent."
  },
  {
    "objectID": "projects/pedestrian-detection.html#next-steps",
    "href": "projects/pedestrian-detection.html#next-steps",
    "title": "Low-Light Pedestrian Detection",
    "section": "Next Steps",
    "text": "Next Steps\n\nTry YOLOv11-s/l and RT-DETR-Tiny for comparison.\n\nImplement soft-NMS and TTA fusion.\n\nExplore confidence calibration and tiling for high-res frames."
  },
  {
    "objectID": "projects/pedestrian-detection.html#repository",
    "href": "projects/pedestrian-detection.html#repository",
    "title": "Low-Light Pedestrian Detection",
    "section": "Repository",
    "text": "Repository\nFull code, scripts, and analysis:\nGitHub ‚Äì lowlight-yolov8-wbf"
  },
  {
    "objectID": "posts/post2/index.html",
    "href": "posts/post2/index.html",
    "title": "Multiple Variable Linear Regression",
    "section": "",
    "text": "In this blog we will try to extend general regression model to support multiple features"
  },
  {
    "objectID": "posts/post2/index.html#pre-requisites",
    "href": "posts/post2/index.html#pre-requisites",
    "title": "Multiple Variable Linear Regression",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nimport copy, math\nimport numpy as np\nimport matplotlib.pyplot as plt\nHere is a summary of some of the notation you will encounter, updated for multiple features. ¬†\n\n\n\n\n\n\n\n\nGeneral Notation\nDescription\nPython (if applicable)\n\n\n\n\n\\(a\\)\nscalar, non bold\n\n\n\n\\(\\mathbf{a}\\)\nvector, bold\n\n\n\n\\(\\mathbf{A}\\)\nmatrix, bold capital\n\n\n\nRegression\n\n\n\n\n\\(\\mathbf{X}\\)\ntraining example matrix\nX_train\n\n\n\\(\\mathbf{y}\\)\ntraining example targets\ny_train\n\n\n\\(\\mathbf{x}^{(i)}\\), \\(y^{(i)}\\)\n\\(i^{th}\\) Training Example\nX[i], y[i]\n\n\n\\(m\\)\nnumber of training examples\nm\n\n\n\\(n\\)\nnumber of features in each example\nn\n\n\n\\(\\mathbf{w}\\)\nparameter: weight\nw\n\n\n\\(b\\)\nparameter: bias\nb\n\n\n\\(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})\\)\nThe result of the model evaluation at \\(\\mathbf{x}^{(i)}\\) parameterized by \\(\\mathbf{w},b\\): \\(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b\\)\nf_wb"
  },
  {
    "objectID": "posts/post2/index.html#problem-statement",
    "href": "posts/post2/index.html#problem-statement",
    "title": "Multiple Variable Linear Regression",
    "section": "2 Problem Statement",
    "text": "2 Problem Statement\nYou will use the motivating example of housing price prediction. The training dataset contains three examples with four features (size, bedrooms, floors and, age) shown in the table below. Note that, unlike the earlier labs, size is in sqft rather than 1000 sqft. This causes an issue, which you will solve in the next lab!\n\n\n\n\n\n\n\n\n\n\nSize (sqft)\nNumber of Bedrooms\nNumber of floors\nAge of Home\nPrice (1000s dollars)\n\n\n\n\n2104\n5\n1\n45\n460\n\n\n1416\n3\n2\n40\n232\n\n\n852\n2\n1\n35\n178\n\n\n\nYou will build a linear regression model using these values so you can then predict the price for other houses. For example, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old.\nPlease run the following code cell to create your X_train and y_train variables.\nX_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\ny_train = np.array([460, 232, 178])\n\n2.1 Matrix X containing our examples\nSimilar to the table above, examples are stored in a NumPy matrix X_train. Each row of the matrix represents one example. When you have \\(m\\) training examples ( \\(m\\) is three in our example), and there are \\(n\\) features (four in our example), \\(\\mathbf{X}\\) is a matrix with dimensions (\\(m\\), \\(n\\)) (m rows, n columns).\n\\[\\mathbf{X} =\n\\begin{pmatrix}\nx^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\\nx^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n\\cdots \\\\\nx^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1}\n\\end{pmatrix}\n\\] notation: - \\(\\mathbf{x}^{(i)}\\) is vector containing example i. \\(\\mathbf{x}^{(i)}\\) $ = (x^{(i)}_0, x^{(i)}1, ,x^{(i)}{n-1})$ - \\(x^{(i)}_j\\) is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.\n\n\n2.2 Parameter vector w, b\n\n\\(\\mathbf{w}\\) is a vector with \\(n\\) elements.\n\nEach element contains the parameter associated with one feature.\nin our dataset, n is 4.\nnotionally, we draw this as a column vector\n\n\n\\[\\mathbf{w} = \\begin{pmatrix}\nw_0 \\\\\nw_1 \\\\\n\\cdots\\\\\nw_{n-1}\n\\end{pmatrix}\n\\] * \\(b\\) is a scalar parameter."
  },
  {
    "objectID": "posts/post2/index.html#model-prediction-with-multiple-variables",
    "href": "posts/post2/index.html#model-prediction-with-multiple-variables",
    "title": "Multiple Variable Linear Regression",
    "section": "3 Model Prediction With Multiple Variables",
    "text": "3 Model Prediction With Multiple Variables\nThe model‚Äôs prediction with multiple variables is given by the linear model:\n\\[ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}\\] or in vector notation: \\[f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b \\tag{2}\\] where \\(\\cdot\\) is a vector dot product.\nTo demonstrate the dot product, we will implement prediction using (1) and (2).\n\n3.1 Single Prediction element by element\nOur previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each element, performing the multiply with its parameter and then adding the bias parameter at the end.\ndef predict_single_loop(x, w, b):\n\n    n = x.shape[0]\n    p = 0\n    for i in range(n):\n        p_i = x[i] * w[i]\n        p = p + p_i\n    p = p + b\n    return p\n\n\n3.2 Single Prediction, vector\nNoting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to speed up predictions.\nRecall from the Python/Numpy lab that NumPy np.dot()[link] can be used to perform a vector dot product.\ndef predict(x, w, b):\n    p = np.dot(x, w) + b\n    return p"
  },
  {
    "objectID": "posts/post2/index.html#compute-cost-with-multiple-variables",
    "href": "posts/post2/index.html#compute-cost-with-multiple-variables",
    "title": "Multiple Variable Linear Regression",
    "section": "4 Compute Cost With Multiple Variables",
    "text": "4 Compute Cost With Multiple Variables\nThe equation for the cost function with multiple variables ( J(,b) ) is:\n\\[\nJ(\\mathbf{w},b) = \\frac{1}{2m} \\sum_{i = 0}^{m-1}\n\\left(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\right)^2 \\tag{3}\n\\]\nwhere:\n\\[\nf_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b \\tag{4}\n\\]\nBelow is an implementation of equations (3) and (4). Note that this uses a standard pattern for this course where a for loop over all m examples is used.\ndef compute_cost(X, y, w, b):\n    m = X.shape[0]\n    cost =0.0\n    for i in range(m):\n        f_wb_i = np.dot(X[i],w) + b\n        cost = cost + (f_wb_i - y[i])**2\n    cost = cost / (2 * m)\n    return cost"
  },
  {
    "objectID": "posts/post2/index.html#gradient-descent-with-multiple-variables",
    "href": "posts/post2/index.html#gradient-descent-with-multiple-variables",
    "title": "Multiple Variable Linear Regression",
    "section": "5 Gradient Descent With Multiple Variables",
    "text": "5 Gradient Descent With Multiple Variables\nGradient descent for multiple variables:\n\\[\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n\\end{align*}\\]\nwhere, n is the number of features, parameters \\(w_j\\), \\(b\\), are updated simultaneously and where\n\\[\n\\begin{align}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n\\end{align}\n\\] * m is the number of training examples in the data set\n\n\\(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})\\) is the model‚Äôs prediction, while \\(y^{(i)}\\) is the target value\n\n\n5.1 Compute Gradient with Multiple Variables\nAn implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an - outer loop over all m examples. - \\(\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}\\) for the example can be computed directly and accumulated - in a second loop over all n features: - \\(\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}\\) is computed for each \\(w_j\\).\ndef compute_gradient(X, y, w, b):\n\n    m,n = X.shape           #(number of examples, number of features)\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    for i in range(m):\n        err = (np.dot(X[i], w) + b) - y[i]\n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err * X[i, j]\n        dj_db = dj_db + err\n    dj_dw = dj_dw / m\n    dj_db = dj_db / m\n\n    return dj_db, dj_dw\n\n\n5.2 Gradient Descent With Multiple Variables\nThe routine below implements equation (5) above.\ndef gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n\n    for i in range(num_iters):\n\n        # Calculate the gradient and update the parameters\n        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               ##None\n        b = b - alpha * dj_db               ##None\n\n        # Save cost J at each iteration\n        if i&lt;100000:      # prevent resource exhaustion\n            J_history.append( cost_function(X, y, w, b))\n\n        # Print cost every at intervals 10 times or as many iterations if &lt; 10\n        if i% math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n\n    return w, b, J_history #return final w,b and J history for graphing"
  },
  {
    "objectID": "posts/post2/index.html#training-code",
    "href": "posts/post2/index.html#training-code",
    "title": "Multiple Variable Linear Regression",
    "section": "Training code",
    "text": "Training code\n# initialize parameters\ninitial_w = np.zeros_like(w_init)\ninitial_b = 0.\n# some gradient descent settings\niterations = 1000\nalpha = 5.0e-7\n# run gradient descent\nw_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n                                                    compute_cost, compute_gradient,\n                                                    alpha, iterations)\nprint(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\nm,_ = X_train.shape\nfor i in range(m):\n    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
  },
  {
    "objectID": "posts/post3/index.html",
    "href": "posts/post3/index.html",
    "title": "Neural Networks for Handwritten Digit Recognition with Binary and Multiple outputs",
    "section": "",
    "text": "In this exercise, you will use a neural network to recognize the hand-written digits zero and one.\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport matplotlib.pyplot as plt\nfrom autils import *\n%matplotlib inline\n\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.autograph.set_verbosity(0)\n\nProblem Statement\nIn this exercise, you will use a neural network to recognize two handwritten digits, zero and one. This is a binary classification task. Automated handwritten digit recognition is widely used today - from recognizing zip codes (postal codes) on mail envelopes to recognizing amounts written on bank checks. You will extend this network to recognize all 10 digits (0-9) in a future assignment.\nThis exercise will show you how the methods you have learned can be used for this classification task.\n\n\nDataset\nYou will start by loading the dataset for this task. - The load_data() function shown below loads the data into variables X and y\n\nThe data set contains 1000 training examples of handwritten digits \\(^1\\), here limited to zero and one.\n\nEach training example is a 20-pixel x 20-pixel grayscale image of the digit.\n\nEach pixel is represented by a floating-point number indicating the grayscale intensity at that location.\nThe 20 by 20 grid of pixels is ‚Äúunrolled‚Äù into a 400-dimensional vector.\nEach training example becomes a single row in our data matrix X.\nThis gives us a 1000 x 400 matrix X where every row is a training example of a handwritten digit image.\n\n\n\n\\[X =\n\\left(\\begin{array}{cc}\n--- (x^{(1)}) --- \\\\\n--- (x^{(2)}) --- \\\\\n\\vdots \\\\\n--- (x^{(m)}) ---\n\\end{array}\\right)\\]\n\nThe second part of the training set is a 1000 x 1 dimensional vector y that contains labels for the training set\n\ny = 0 if the image is of the digit 0, y = 1 if the image is of the digit 1.\n\n\n\\(^1\\) This is a subset of the MNIST handwritten digit dataset (http://yann.lecun.com/exdb/mnist/)\nYou will begin by visualizing a subset of the training set. - In the cell below, the code randomly selects 64 rows from X, maps each row back to a 20 pixel by 20 pixel grayscale image and displays the images together. - The label for each image is displayed above the image\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(8,8))\nfig.tight_layout(pad=0.1)\n\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n\n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n\n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n\n    # Display the label above the image\n    ax.set_title(y[random_index,0])\n    ax.set_axis_off()\n\nThe parameters have dimensions that are sized for a neural network with \\(25\\) units in layer 1, \\(15\\) units in layer 2 and \\(1\\) output unit in layer 3.\n\nRecall that the dimensions of these parameters are determined as follows:\n\nIf network has \\(s_{in}\\) units in a layer and \\(s_{out}\\) units in the next layer, then\n\n\\(W\\) will be of dimension \\(s_{in} \\times s_{out}\\).\n\\(b\\) will a vector with \\(s_{out}\\) elements\n\n\nTherefore, the shapes of W, and b, are\n\nlayer1: The shape of W1 is (400, 25) and the shape of b1 is (25,)\nlayer2: The shape of W2 is (25, 15) and the shape of b2 is: (15,)\nlayer3: The shape of W3 is (15, 1) and the shape of b3 is: (1,) &gt;Note: The bias vector b could be represented as a 1-D (n,) or 2-D (1,n) array. Tensorflow utilizes a 1-D representation and this lab will maintain that convention.\n\n\n\n\n\nBinary output\nmodel = Sequential(\n    [\n        tf.keras.Input(shape=(400,)),\n        Dense(25, activation = 'sigmoid')\n        Dense(15, activation = 'sigmoid')\n        Dense(1, activation = 'sigmoid')\n    ], name = \"my_model\"\n)\nThe following code will define a loss function and run gradient descent to fit the weights of the model to the training data.\nmodel.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.001),\n)\n\nmodel.fit(\n    X,y,\n    epochs=20\n)\nLet‚Äôs compare the predictions vs the labels for a random sample of 64 digits. This takes a moment to run.\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# You do not need to modify anything in this cell\n\nm, n = X.shape\n\nfig, axes = plt.subplots(8,8, figsize=(8,8))\nfig.tight_layout(pad=0.1,rect=[0, 0.03, 1, 0.92]) #[left, bottom, right, top]\n\nfor i,ax in enumerate(axes.flat):\n    # Select random indices\n    random_index = np.random.randint(m)\n\n    # Select rows corresponding to the random indices and\n    # reshape the image\n    X_random_reshaped = X[random_index].reshape((20,20)).T\n\n    # Display the image\n    ax.imshow(X_random_reshaped, cmap='gray')\n\n    # Predict using the Neural Network\n    prediction = model.predict(X[random_index].reshape(1,400))\n    if prediction &gt;= 0.5:\n        yhat = 1\n    else:\n        yhat = 0\n\n    # Display the label above the image\n    ax.set_title(f\"{y[random_index,0]},{yhat}\")\n    ax.set_axis_off()\nfig.suptitle(\"Label, yhat\", fontsize=16)\nplt.show()\n\n\nMultiple output\nNow we will extend that to multiclass classification. This will utilize the softmax activation.\nNumerical stability is improved if the softmax is grouped with the loss function rather than the output layer during training. This has implications when building the model and using the model. Building: * The final Dense layer should use a ‚Äòlinear‚Äô activation. This is effectively no activation. * The model.compile statement will indicate this by including from_logits=True. loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) * This does not impact the form of the target. In the case of SparseCategorialCrossentropy, the target is the expected digit, 0-9.\nUsing the model: * The outputs are not probabilities. If output probabilities are desired, apply a softmax function.\ntf.random.set_seed(1234)\nmodel = Sequential(\n    [\n        tf.keras.Input(shape=(400,)),     # @REPLACE\n        Dense(25, activation='relu', name = \"L1\"), # @REPLACE\n        Dense(15, activation='relu',  name = \"L2\"), # @REPLACE\n        Dense(10, activation='linear', name = \"L3\"),  # @REPLACE\n    ], name = \"my_model\"\n)\nmodel.compile(\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n)\n\nhistory = model.fit(\n    X,y,\n    epochs=40\n)\n\nprediction = model.predict(image_of_two.reshape(1,400))  # prediction\n\nprint(f\" predicting a Two: \\n{prediction}\")\nprint(f\" Largest Prediction index: {np.argmax(prediction)}\")\nTo return an integer representing the predicted target, you want the index of the largest probability. This is accomplished with the Numpy argmax function.\nyhat = np.argmax(prediction_p)\n\nprint(f\"np.argmax(prediction_p): {yhat}\")"
  },
  {
    "objectID": "posts/post5/index.html",
    "href": "posts/post5/index.html",
    "title": "Anomaly detection",
    "section": "",
    "text": "In this tutorial, we will implement the anomaly detection algorithm and apply it to detect failing servers on a network.\n\nProblem Statement\nThe dataset contains two features - * throughput (mb/s) and * latency (ms) of response of each server.\nWhile your servers were operating, you collected \\(m=307\\) examples of how they were behaving, and thus have an unlabeled dataset \\[\n\\{x^{(1)}, \\ldots, x^{(m)}\\}\n\\]. * You suspect that the vast majority of these examples are ‚Äúnormal‚Äù (non-anomalous) examples of the servers operating normally, but there might also be some examples of servers acting anomalously within this dataset.\nYou will use a Gaussian model to detect anomalous examples in your dataset. * You will first start on a 2D dataset that will allow you to visualize what the algorithm is doing. * On that dataset you will fit a Gaussian distribution and then find values that have very low probability and hence can be considered anomalies. * After that, you will apply the anomaly detection algorithm to a larger dataset with many dimensions.\n- You will use `X_train` to fit a Gaussian distribution\n- You will use `X_val` and `y_val` as a cross validation set to select a threshold and determine anomalous vs normal examples\n\n\nGaussian distribution\nTo perform anomaly detection, you will first need to fit a model to the data‚Äôs distribution.\n\nGiven a training set \\[\n\\{x^{(1)}, ..., x^{(m)}\\}\n\\] you want to estimate the Gaussian distribution for each of the features \\(x_i\\).\nRecall that the Gaussian distribution is given by\n\\[\np(x ; \\mu,\\sigma ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^2}}\\exp^{ - \\frac{(x - \\mu)^2}{2 \\sigma ^2} }\n\\]\nwhere \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance.\nFor each feature \\(i = 1\\ldots n\\), you need to find parameters \\(\\mu_i\\) and \\(\\sigma_i^2\\) that fit the data in the \\(i\\)-th dimension \\(\\{x_i^{(1)}, ..., x_i^{(m)}\\}\\) (the \\(i\\)-th dimension of each example).\n\n\nEstimating parameters for a Gaussian distribution\nYou can estimate the parameters, (\\(\\mu_i\\), \\(\\sigma_i^2\\)), of the \\(i\\)-th feature by using the following equations. To estimate the mean, you will use:\n\\[\n\\mu_i = \\frac{1}{m} \\sum_{j=1}^m x_i^{(j)\\]\nand for the variance you will use: \\[\n\\sigma_i^2 = \\frac{1}{m} \\sum_{j=1}^m (x_i^{(j)} - \\mu_i)^2\n\\]\ndef estimate_gaussian(X):\n    m, n = X.shape\n\n    mu = (1/m)*np.sum(X, axis=0)\n    var = (1/m)*np.sum((X-mu)**2,axis=0)\n    return mu, var\n\n\nSelecting the threshold \\(\\epsilon\\)\nNow that you have estimated the Gaussian parameters, you can investigate which examples have a very high probability given this distribution and which examples have a very low probability.\n\nThe low probability examples are more likely to be the anomalies in our dataset.\nOne way to determine which examples are anomalies is to select a threshold based on a cross validation set.\n\nIn this section, you will complete the code in select_threshold to select the threshold \\(\\varepsilon\\) using the \\(F_1\\) score on a cross validation set.\n\nFor this, we will use a cross validation set \\[\n\\{(x_{\\rm cv}^{(1)}, y_{\\rm cv}^{(1)}),\\ldots, (x_{\\rm cv}^{(m_{\\rm cv})}, y_{\\rm cv}^{(m_{\\rm cv})})\\}\n\\] , where the label \\(y=1\\) corresponds to an anomalous example, and \\(y=0\\) corresponds to a normal example.\nFor each cross validation example, we will compute \\(p(x_{\\rm cv}^{(i)})\\). The vector of all of these probabilities \\(p(x_{\\rm cv}^{(1)}), \\ldots, p(x_{\\rm cv}^{(m_{\\rm cv})})\\) is passed to select_threshold in the vector p_val.\nThe corresponding labels \\(y_{\\rm cv}^{(1)}, \\ldots, y_{\\rm cv}^{(m_{\\rm cv})}\\) are passed to the same function in the vector y_val.\nIn the provided code select_threshold, there is already a loop that will try many different values of \\(\\varepsilon\\) and select the best \\(\\varepsilon\\) based on the \\(F_1\\) score.\nYou need to implement code to calculate the F1 score from choosing epsilon as the threshold and place the value in F1.\n\nRecall that if an example \\(x\\) has a low probability \\(p(x) &lt; \\varepsilon\\), then it is classified as an anomaly.\nThen, you can compute precision and recall by: \\[\n\\begin{aligned}\nprec&=&\\frac{tp}{tp+fp}\\\\\nrec&=&\\frac{tp}{tp+fn},\n\\end{aligned}\n\\] where\n\n\\(tp\\) is the number of true positives: the ground truth label says it‚Äôs an anomaly and our algorithm correctly classified it as an anomaly.\n\\(fp\\) is the number of false positives: the ground truth label says it‚Äôs not an anomaly, but our algorithm incorrectly classified it as an anomaly.\n\\(fn\\) is the number of false negatives: the ground truth label says it‚Äôs an anomaly, but our algorithm incorrectly classified it as not being anomalous.\n\nThe \\(F_1\\) score is computed using precision (\\(prec\\)) and recall (\\(rec\\)) as follows: \\[\nF_1 = \\frac{2\\cdot prec \\cdot rec}{prec + rec}\n\\]\n\n\ndef select_threshold(y_val, p_val):\n    best_epsilon = 0\n    best_F1 = 0\n    F1 = 0\n    step_size = (max(p_val) - min(p_val)) / 1000\n    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n\n        predictions = (p_val &lt; epsilon)\n\n        tp = np.sum((predictions == 1) & (y_val == 1))\n        fp = np.sum((predictions == 1) & (y_val == 0))\n        fn = np.sum((predictions == 0) & (y_val == 1))\n\n        prec = tp / (tp + fp)\n        rec = tp / (tp + fn)\n        F1 = 2 * prec * rec / (prec + rec)\n        if F1 &gt; best_F1:\n            best_F1 = F1\n            best_epsilon = epsilon\n    return best_epsilon, best_F1"
  },
  {
    "objectID": "posts/post4/index.html",
    "href": "posts/post4/index.html",
    "title": "Building a Decision Tree from scratch",
    "section": "",
    "text": "Suppose you are starting a company that grows and sells wild mushrooms. - Since not all mushrooms are edible, you‚Äôd like to be able to tell whether a given mushroom is edible or poisonous based on it‚Äôs physical attributes - You have some existing data that you can use for this task.\nCan you use the data to help you identify which mushrooms can be sold safely?\nNote: The dataset used is for illustrative purposes only. It is not meant to be a guide on identifying edible mushrooms."
  },
  {
    "objectID": "posts/post4/index.html#decision-tree",
    "href": "posts/post4/index.html#decision-tree",
    "title": "Building a Decision Tree from scratch",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nSteps for building a decision tree are as follows:\n\nStart with all examples at the root node\nCalculate information gain for splitting on all possible features, and pick the one with the highest information gain\nSplit dataset according to the selected feature, and create left and right branches of the tree\nKeep repeating splitting process until stopping criteria is met\n\nIn this tutorial, we‚Äôll implement the following functions, which will let you split a node into left and right branches using the feature with the highest information gain\n\nCalculate the entropy at a node\nSplit the dataset at a node into left and right branches based on a given feature\nCalculate the information gain from splitting on a given feature\nChoose the feature that maximizes information gain"
  },
  {
    "objectID": "posts/post4/index.html#calculate-entropy",
    "href": "posts/post4/index.html#calculate-entropy",
    "title": "Building a Decision Tree from scratch",
    "section": "Calculate entropy",
    "text": "Calculate entropy\nFirst, we‚Äôll write a helper function called compute_entropy that computes the entropy (measure of impurity) at a node. - The function takes in a numpy array (y) that indicates whether the examples in that node are edible (1) or poisonous(0)\nComplete the compute_entropy() function below to: * Compute \\(p_1\\), which is the fraction of examples that are edible (i.e.¬†have value = 1 in y) * The entropy is then calculated as\n\\[H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)\\] * Note * The log is calculated with base \\(2\\) * For implementation purposes, \\(0\\text{log}_2(0) = 0\\). That is, if p_1 = 0 or p_1 = 1, set the entropy to 0 * Make sure to check that the data at a node is not empty (i.e.¬†len(y) != 0). Return 0 if it is\ndef compute_entropy(y):\n    entropy = 0.\n    if len(y) != 0:\n        # Fraction of positive examples (label = 1)\n        p1 = len(y[y == 1]) / len(y)\n\n        # Handle edge cases where p1 = 0 or 1 (avoid log(0))\n        if p1 != 0 and p1 != 1:\n            entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n        else:\n            entropy = 0.\n    return entropy\n\nSplit dataset\nNext, we‚Äôll write a helper function called split_dataset that takes in the data at a node and a feature to split on and splits it into left and right branches. Later in the lab, you‚Äôll implement code to calculate how good the split is.\n\nThe function takes in the training data, the list of indices of data points at that node, along with the feature to split on.\nIt splits the data and returns the subset of indices at the left and the right branch.\nFor example, say we‚Äôre starting at the root node (so node_indices = [0,1,2,3,4,5,6,7,8,9]), and we chose to split on feature 0, which is whether or not the example has a brown cap.\n\nThe output of the function is then, left_indices = [0,1,2,3,4,7,9] (data points with brown cap) and right_indices = [5,6,8] (data points without a brown cap)\n\n\ndef split_dataset(X, node_indices, feature):\n        # You need to return the following variables correctly\n        left_indices = []\n        right_indices = []\n        # Go through the indices of examples at that node\n        for i in node_indices:\n            if # Your code here to check if the value of X at that index for the feature is 1\n                left_indices.append(i)\n            else:\n                right_indices.append(i)\n    return left_indices, right_indices\n\n\nCalculate information gain\nNext, we‚Äôll write a function called information_gain that takes in the training data, the indices at a node and a feature to split on and returns the information gain from the split.\n\\[\\text{Information Gain} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))\\]\nwhere - \\(H(p_1^\\text{node})\\) is entropy at the node - \\(H(p_1^\\text{left})\\) and \\(H(p_1^\\text{right})\\) are the entropies at the left and the right branches resulting from the split - \\(w^{\\text{left}}\\) and \\(w^{\\text{right}}\\) are the proportion of examples at the left and right branch, respectively\ndef compute_information_gain(X, y, node_indices, feature):\n    # Split dataset\n    left_indices, right_indices = split_dataset(X, node_indices, feature)\n\n    # Extract data for node and its branches\n    y_node = y[node_indices]\n    y_left = y[left_indices]\n    y_right = y[right_indices]\n    # Entropy at the current node\n    H_node = compute_entropy(y_node)\n\n    # Entropy at left and right branches\n    H_left = compute_entropy(y_left)\n    H_right = compute_entropy(y_right)\n\n    # Compute proportions of samples on each side\n    w_left = len(y_left) / len(y_node)\n    w_right = len(y_right) / len(y_node)\n\n    # Compute information gain\n    information_gain = H_node - (w_left * H_left + w_right * H_right)\n    return information_gain\n\n\nGet best split\nNow let‚Äôs write a function to get the best feature to split on by computing the information gain from each feature as we did above and returning the feature that gives the maximum information gain\ndef get_best_split(X, y, node_indices):\n    num_features = X.shape[1]\n    best_feature = -1\n    max_info_gain = -1  # keep track of best score\n\n    for feature in range(num_features):\n        info_gain = compute_information_gain(X, y, node_indices, feature)\n\n        if info_gain &gt; max_info_gain:\n            max_info_gain = info_gain\n            best_feature = feature\n    return best_feature\n\n\nBuilding the tree\nIn this section, we use the functions we implemented above to generate a decision tree by successively picking the best feature to split on until we reach the stopping criteria (maximum depth is 2).\n# Not graded\ntree = []\n\ndef build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n    # Maximum depth reached - stop splitting\n    if current_depth == max_depth:\n        formatting = \" \"*current_depth + \"-\"*current_depth\n        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n        return\n\n    # Otherwise, get best split and split the data\n    # Get the best feature and threshold at this node\n    best_feature = get_best_split(X, y, node_indices)\n\n    formatting = \"-\"*current_depth\n    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n\n    # Split the dataset at the best feature\n    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n    tree.append((left_indices, right_indices, best_feature))\n\n    # continue splitting the left and the right child. Increment current depth\n    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)\nUsage example:\nbuild_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Finley Malloc is the Chief Data Scientist at Wengo Analytics. When not innovating on data platforms, Finley enjoys spending time unicycling and playing with her pet iguana.\n\n\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St.¬†Paul, MN B.A in Economics | Sept 2007 - June 2011\n\n\n\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St.¬†Paul, MN B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  }
]